{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMweLRJPOXHD3NrJicHdAf5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-AusIzTl4kF"
      },
      "outputs": [],
      "source": [
        "# stream_to_kinesis.py\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"CloudWatch_Usage_With_Timestamp_Full.csv\")\n",
        "\n",
        "# Kinesis setup\n",
        "kinesis = boto3.client('kinesis', region_name='us-east-1')  # update region if needed\n",
        "stream_name = \"cloudwatch-metrics-stream\"\n",
        "\n",
        "# Feature selection\n",
        "features = [\n",
        "    'CPUUtilization', 'MemoryUtilization', 'DiskReadBytes_MB', 'DiskWriteBytes_MB',\n",
        "    'NetworkIn_MB', 'NetworkOut_MB', 'Latency_ms', 'DiskQueueLength',\n",
        "    'RunningProcesses', 'ActiveConnections'\n",
        "]\n",
        "\n",
        "# Stream each row to Kinesis\n",
        "for _, row in df.iterrows():\n",
        "    record = row[features].to_dict()\n",
        "    kinesis.put_record(\n",
        "        StreamName=stream_name,\n",
        "        Data=json.dumps(record),\n",
        "        PartitionKey=\"partitionKey\"\n",
        "    )\n",
        "    print(\"Sent:\", record)\n",
        "    time.sleep(0.5)  # simulate real-time feed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lambda_function.py\n",
        "import json\n",
        "import boto3\n",
        "import joblib\n",
        "import numpy as np\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Load model & scaler from /opt (Lambda Layer) or S3\n",
        "s3 = boto3.client('s3')\n",
        "bucket = 'your-model-bucket'\n",
        "model_key = 'kmeans_model.pkl'\n",
        "scaler_key = 'scaler.pkl'\n",
        "\n",
        "model_file = BytesIO(s3.get_object(Bucket=bucket, Key=model_key)['Body'].read())\n",
        "scaler_file = BytesIO(s3.get_object(Bucket=bucket, Key=scaler_key)['Body'].read())\n",
        "\n",
        "model = joblib.load(model_file)\n",
        "scaler = joblib.load(scaler_file)\n",
        "\n",
        "# Connect to DynamoDB\n",
        "dynamodb = boto3.resource('dynamodb')\n",
        "table = dynamodb.Table('AnomalyResults')\n",
        "\n",
        "# List of features used\n",
        "features = [\n",
        "    'CPUUtilization', 'MemoryUtilization', 'DiskReadBytes_MB', 'DiskWriteBytes_MB',\n",
        "    'NetworkIn_MB', 'NetworkOut_MB', 'Latency_ms', 'DiskQueueLength',\n",
        "    'RunningProcesses', 'ActiveConnections'\n",
        "]\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    for record in event['Records']:\n",
        "        payload = json.loads(record['kinesis']['data'])\n",
        "\n",
        "        # Convert input to model format\n",
        "        X = np.array([[payload.get(f, 0) for f in features]])\n",
        "        X_scaled = scaler.transform(X)\n",
        "        label = int(model.predict(X_scaled)[0])\n",
        "\n",
        "        # Add cluster label to payload\n",
        "        payload['ClusterLabel'] = label\n",
        "\n",
        "        # Save to DynamoDB\n",
        "        table.put_item(Item=payload)\n",
        "\n",
        "    return {'statusCode': 200, 'body': 'Processed batch successfully'}"
      ],
      "metadata": {
        "id": "myPbzfx5l6rN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
